{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1atQIthOqnop",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train a model with Classical Training\n",
    "\n",
    "Although episodic training has attracted a lot of interest in the early years of Few-Shot Learning research, more recent works suggest that competitive results can be achieved with a simple cross entropy loss across all training classes. Therefore, it is becoming more and more common to use this classical process to train the backbone, that will be common to all methods compared at test time.\n",
    "\n",
    "This is in fact more representative of real use cases: episodic training assumes that, at training time, you have access to the shape of the few-shot tasks that will be encountered at test time (indeed you choose a specific number of ways for episodic training). You also \"force\" your inference method into the training of the network. Switching the few-shot learning logic to inference (i.e. no episodic training) allows methods to be agnostic of the backbone.\n",
    "\n",
    "Nonetheless, if you need to perform episodic training, we also provide [an example notebook](episodic_training.ipynb) for that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "axL7cV71qnoz",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Getting started\n",
    "First we're going to do some imports (this is not the interesting part)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "gZ8bXCqpqno0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "50xGcwMrqno3",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we're gonna do the most important thing in Machine Learning research: ensuring reproducibility by setting the random seed. We're going to set the seed for all random packages that we could possibly use, plus some other stuff to make CUDA deterministic (see [here](https://pytorch.org/docs/stable/notes/randomness.html)).\n",
    "\n",
    "I strongly encourage that you do this in **all your scripts**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "vpqXEWt8qno4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "random_seed = 0\n",
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "random.seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xW257QB4qno5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we're gonna create our data loader for the training set. You can see that I chose tu use CUB in this notebook, because it’s a small dataset, so we can have good results quite quickly. I set a batch size of 128 but feel free to adapt it to your constraints.\n",
    "\n",
    "Note that we're not using the `TaskSampler` for the train data loader, because we won't be sampling training data in the shape of tasks as we would have in episodic training. We do it **normally**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3kYxNc4tqno7",
    "outputId": "85a76b58-4267-44dc-85fd-e9fff2243563",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/UCMerced-Fewshot directory exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrolling dataset's labels...: 100%|██████████| 1000/1000 [00:00<00:00, 1011.91it/s]\n",
      "Scrolling dataset's labels...: 100%|██████████| 500/500 [00:00<00:00, 1084.30it/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from wrap_few_shot_dataset import WrapFewShotDataset\n",
    "\n",
    "batch_size = 128\n",
    "n_workers = 8\n",
    "\n",
    "# Setup path to data folder\n",
    "data_path = Path(\"data\")\n",
    "image_path = data_path / \"UCMerced-Fewshot\"\n",
    "\n",
    "# Check if image folder exists\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory\")\n",
    "    exit()\n",
    "\n",
    "# Setup train and testing paths\n",
    "train_dir = image_path / \"Train\"\n",
    "val_dir = image_path / \"Val\"\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(128),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "train_set = datasets.ImageFolder(\n",
    "    root=train_dir,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "val_set = datasets.ImageFolder(\n",
    "    root=val_dir,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "train_set = WrapFewShotDataset(train_set)\n",
    "val_set = WrapFewShotDataset(val_set)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G09f8Rkiqno8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we are going to create the model that we want to train. Here we choose the ResNet12 that is very often used in Few-Shot Learning research. Note that the default setting of these networks in EasyFSL is to not have a last fully connected layer (as it is usual for most Few-Shot Learning methods), but for classical training we need this layer! We also force it to output a vector which size is the number of different classes in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-zNE9ffbqnpA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from modules.predefined_resnet import resnet12\n",
    "\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "model = resnet12(\n",
    "    use_fc=True,\n",
    "    num_classes=len(set(train_set.get_labels())),\n",
    ").to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5uP_qztRqnpB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now, we still need validation ! Since we're training a model to perform few-shot classification, we will validate on few-shot tasks, so now we'll use the `TaskSampler`. We arbitrarily set the shape of the validation tasks. Ideally, you'd like to perform validation on various shapes of tasks, but we didn't implement this yet (feel free to contribute!).\n",
    "\n",
    "We also need to define the few-shot classification method that we will use during validation of the neural network we're training.\n",
    "Here we choose Prototypical Networks, because it's simple and efficient, but this is still an arbitrary choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DrUD75fEqnpC",
    "outputId": "fb3cd669-7cbb-40c8-9059-3ba359c96ac9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from fewshot_sampler import FewShotSampler\n",
    "\n",
    "n_way = 5\n",
    "n_shot = 5\n",
    "n_query = 10\n",
    "n_validation_tasks = 50\n",
    "\n",
    "val_sampler = FewShotSampler(\n",
    "    val_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_validation_tasks\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_sampler=val_sampler,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=val_sampler.episodic_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick one of the following models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.prototypical import PrototypicalNetworks\n",
    "\n",
    "few_shot_classifier = PrototypicalNetworks(model).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules.simple_shot import SimpleShot\n",
    "\n",
    "few_shot_classifier = SimpleShot(model).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "few_shot_classifier = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8DSkTh4zqnpD",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Training\n",
    "\n",
    "Now let's define our training helpers ! I chose to use Stochastic Gradient Descent on 200 epochs with a scheduler that divides the learning rate by 10 after 120 and 160 epochs. The strategy is derived from [this repo](https://github.com/fiveai/on-episodes-fsl).\n",
    "\n",
    "We're also gonna use a TensorBoard because it's always good to see what your training curves look like.\n",
    "\n",
    "An other thing: we're doing 200 epochs like in [the episodic training notebook](notebooks/episodic_training.ipynb), but keep in mind that an epoch in classical training means one pass through the 6000 images of the dataset, while in episodic training it's an arbitrary number of episodes. In the episodic training notebook an epoch is 500 episodes of 5-way, 5-shot, 10-query tasks, so 37500 images. TL;DR you may want to monitor your training and increase the number of epochs if necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "59DHe5LBqnpE",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD, Optimizer\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "LOSS_FUNCTION = nn.CrossEntropyLoss()\n",
    "\n",
    "n_epochs = 100\n",
    "scheduler_milestones = [150, 180]\n",
    "scheduler_gamma = 0.1\n",
    "learning_rate = 1e-01\n",
    "tb_logs_dir = Path(\".\")\n",
    "\n",
    "train_optimizer = SGD(\n",
    "    model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=5e-4\n",
    ")\n",
    "train_scheduler = MultiStepLR(\n",
    "    train_optimizer,\n",
    "    milestones=scheduler_milestones,\n",
    "    gamma=scheduler_gamma,\n",
    ")\n",
    "\n",
    "tb_writer = SummaryWriter(log_dir=str(tb_logs_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3RgyjdOUqnpF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And now let's get to it! Here we define the function that performs a training epoch.\n",
    "\n",
    "We use tqdm to monitor the training in real time in our logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "NxMeTTCwqnpF",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def training_epoch(model_: nn.Module, data_loader: DataLoader, optimizer: Optimizer):\n",
    "    all_loss = []\n",
    "    model_.train()\n",
    "    with tqdm(data_loader, total=len(data_loader), desc=\"Training\") as tqdm_train:\n",
    "        for images, labels in tqdm_train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            loss = LOSS_FUNCTION(model_(images.to(DEVICE)), labels.to(DEVICE))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            all_loss.append(loss.item())\n",
    "\n",
    "            tqdm_train.set_postfix(loss=mean(all_loss))\n",
    "\n",
    "    return mean(all_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1P6GlRAqnpG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "And we have everything we need! This is now the time to **start training**.\n",
    "\n",
    "A few notes:\n",
    "\n",
    "- We only validate every 10 epochs (you may set an even less frequent validation) because a training epoch is much faster than 500 few-shot tasks, and we don't want validation to be the bottleneck of our training process.\n",
    "\n",
    "- I also added something to log the state of the model that gave the best performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QEm3sKqPqnpG",
    "outputId": "680e0012-3e7b-470d-f800-cbd75e41f773",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:57<00:00, 29.68s/it, loss=2.32]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:57<00:00, 29.70s/it, loss=1.85]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:59<00:00, 29.92s/it, loss=1.3] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:56<00:00, 29.55s/it, loss=1.26]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:04<00:00, 30.61s/it, loss=1.28]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:03<00:00, 30.49s/it, loss=1.06]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:08<00:00, 31.12s/it, loss=0.942]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:02<00:00, 30.31s/it, loss=0.866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:02<00:00, 30.26s/it, loss=0.808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:02<00:00, 30.26s/it, loss=0.832]\n",
      "Validation: 100%|██████████| 50/50 [04:46<00:00,  5.73s/it, accuracy=0.38] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:09<00:00, 31.23s/it, loss=0.806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:01<00:00, 30.24s/it, loss=0.648]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:01<00:00, 30.22s/it, loss=0.63] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:00<00:00, 30.02s/it, loss=0.651]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:01<00:00, 30.21s/it, loss=0.654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:58<00:00, 29.81s/it, loss=0.624]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:00<00:00, 30.11s/it, loss=0.576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:07<00:00, 30.89s/it, loss=0.524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:00<00:00, 30.01s/it, loss=0.592]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:07<00:00, 30.92s/it, loss=0.528]\n",
      "Validation: 100%|██████████| 50/50 [04:41<00:00,  5.62s/it, accuracy=0.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:56<00:00, 29.56s/it, loss=0.48] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:10<00:00, 31.34s/it, loss=0.507]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:01<00:00, 30.15s/it, loss=0.561]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:03<00:00, 30.40s/it, loss=0.485]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:02<00:00, 30.36s/it, loss=0.441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:00<00:00, 30.03s/it, loss=0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:00<00:00, 30.08s/it, loss=0.443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:08<00:00, 31.07s/it, loss=0.472]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:05<00:00, 30.65s/it, loss=0.427]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:03<00:00, 30.46s/it, loss=0.42] \n",
      "Validation: 100%|██████████| 50/50 [04:46<00:00,  5.72s/it, accuracy=0.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:00<00:00, 30.08s/it, loss=0.349]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:03<00:00, 30.48s/it, loss=0.362]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:05<00:00, 30.64s/it, loss=0.425]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:11<00:00, 31.47s/it, loss=0.345]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:04<00:00, 30.57s/it, loss=0.393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:08<00:00, 31.04s/it, loss=0.342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:16<00:00, 32.10s/it, loss=0.357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:14<00:00, 31.81s/it, loss=0.317]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:04<00:00, 30.52s/it, loss=0.313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:01<00:00, 30.22s/it, loss=0.342]\n",
      "Validation: 100%|██████████| 50/50 [04:46<00:00,  5.73s/it, accuracy=0.509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:01<00:00, 30.18s/it, loss=0.286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:01<00:00, 30.23s/it, loss=0.334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:00<00:00, 30.09s/it, loss=0.269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:10<00:00, 31.25s/it, loss=0.313]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:07<00:00, 30.97s/it, loss=0.302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:58<00:00, 29.79s/it, loss=0.269]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:02<00:00, 30.33s/it, loss=0.283]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:59<00:00, 29.96s/it, loss=0.204]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:59<00:00, 29.88s/it, loss=0.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:59<00:00, 30.00s/it, loss=0.251]\n",
      "Validation: 100%|██████████| 50/50 [04:45<00:00,  5.71s/it, accuracy=0.542]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:06<00:00, 30.77s/it, loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:02<00:00, 30.33s/it, loss=0.289]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:03<00:00, 30.48s/it, loss=0.26] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:02<00:00, 30.35s/it, loss=0.253]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:04<00:00, 30.53s/it, loss=0.261]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:02<00:00, 30.31s/it, loss=0.259]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:58<00:00, 29.84s/it, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:00<00:00, 30.00s/it, loss=0.208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:00<00:00, 30.03s/it, loss=0.209]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:01<00:00, 30.21s/it, loss=0.21] \n",
      "Validation: 100%|██████████| 50/50 [04:43<00:00,  5.67s/it, accuracy=0.528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:04<00:00, 30.51s/it, loss=0.231]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:01<00:00, 30.20s/it, loss=0.224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:01<00:00, 30.22s/it, loss=0.239]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:03<00:00, 30.42s/it, loss=0.201]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:03<00:00, 30.46s/it, loss=0.203]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:03<00:00, 30.42s/it, loss=0.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:04<00:00, 30.54s/it, loss=0.196]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:06<00:00, 30.80s/it, loss=0.178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:02<00:00, 30.27s/it, loss=0.152]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:06<00:00, 30.87s/it, loss=0.182] \n",
      "Validation: 100%|██████████| 50/50 [04:39<00:00,  5.59s/it, accuracy=0.455]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:57<00:00, 29.72s/it, loss=0.158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:07<00:00, 30.88s/it, loss=0.15] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:07<00:00, 30.89s/it, loss=0.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:01<00:00, 30.21s/it, loss=0.238]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:01<00:00, 30.16s/it, loss=0.215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:00<00:00, 30.01s/it, loss=0.18] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:57<00:00, 29.66s/it, loss=0.186]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:59<00:00, 29.90s/it, loss=0.166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:58<00:00, 29.77s/it, loss=0.197]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:59<00:00, 29.91s/it, loss=0.204]\n",
      "Validation: 100%|██████████| 50/50 [04:44<00:00,  5.70s/it, accuracy=0.613]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:06<00:00, 30.78s/it, loss=0.128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:01<00:00, 30.15s/it, loss=0.129]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:02<00:00, 30.31s/it, loss=0.171]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:03<00:00, 30.47s/it, loss=0.188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:05<00:00, 30.66s/it, loss=0.184]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:00<00:00, 30.08s/it, loss=0.15] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:04<00:00, 30.51s/it, loss=0.158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:04<00:00, 30.57s/it, loss=0.181]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:00<00:00, 30.11s/it, loss=0.139] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:03<00:00, 30.44s/it, loss=0.155]\n",
      "Validation: 100%|██████████| 50/50 [04:49<00:00,  5.78s/it, accuracy=0.627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ding ding ding! We found a new best model!\n",
      "Epoch 90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:57<00:00, 29.70s/it, loss=0.168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:59<00:00, 29.97s/it, loss=0.135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:05<00:00, 30.73s/it, loss=0.143] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [03:58<00:00, 29.82s/it, loss=0.144]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:03<00:00, 30.43s/it, loss=0.156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:01<00:00, 30.22s/it, loss=0.163] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 96\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:02<00:00, 30.32s/it, loss=0.15]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 97\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:03<00:00, 30.42s/it, loss=0.168]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:03<00:00, 30.47s/it, loss=0.139] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 8/8 [04:00<00:00, 30.00s/it, loss=0.128]\n",
      "Validation: 100%|██████████| 50/50 [04:47<00:00,  5.75s/it, accuracy=0.499]\n"
     ]
    }
   ],
   "source": [
    "from fewshot_utils import evaluate\n",
    "\n",
    "\n",
    "best_state = model.state_dict()\n",
    "best_validation_accuracy = 0.0\n",
    "validation_frequency = 10\n",
    "for epoch in range(n_epochs):\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    average_loss = training_epoch(model, train_loader, train_optimizer)\n",
    "\n",
    "    if epoch % validation_frequency == validation_frequency - 1:\n",
    "\n",
    "        # We use this very convenient method from EasyFSL's ResNet to specify\n",
    "        # that the model shouldn't use its last fully connected layer during validation.\n",
    "        model.set_use_fc(False)\n",
    "        validation_accuracy = evaluate(\n",
    "            few_shot_classifier, val_loader, device=DEVICE, tqdm_prefix=\"Validation\"\n",
    "        )\n",
    "        model.set_use_fc(True)\n",
    "\n",
    "        if validation_accuracy > best_validation_accuracy:\n",
    "            best_validation_accuracy = validation_accuracy\n",
    "            best_state = model.state_dict()\n",
    "            print(\"Ding ding ding! We found a new best model!\")\n",
    "\n",
    "        tb_writer.add_scalar(\"Val/acc\", validation_accuracy, epoch)\n",
    "\n",
    "    tb_writer.add_scalar(\"Train/loss\", average_loss, epoch)\n",
    "\n",
    "    # Warn the scheduler that we did an epoch\n",
    "    # so it knows when to decrease the learning rate\n",
    "    train_scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHWLcJ99qnpH",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Yay we successfully performed Classical Training! Now if you want to you can retrieve the best model's state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EG9bBTz4qnpH",
    "outputId": "f4aed221-a7fc-4563-d51c-d6606fd704ac",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(best_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(few_shot_classifier.state_dict(),\"models/fewshot_merced_proto_scratch.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l0iEJFIiqnpI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Evaluation\n",
    "\n",
    "Now that our model is trained, we want to test it.\n",
    "\n",
    "First step: we fetch the test data. Note that we'll evaluate on the same shape of tasks as in validation. This is malicious practice, because it means that we used *a priori* information about the evaluation tasks during training. This is still less malicious than episodic training, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eveON7-wqnpI",
    "outputId": "14f75db7-8395-4019-ef4a-73d8ce72bca9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scrolling dataset's labels...: 100%|██████████| 900/900 [00:01<00:00, 694.93it/s]\n"
     ]
    }
   ],
   "source": [
    "n_test_tasks = 1000\n",
    "\n",
    "test_dir = image_path / \"Test\"\n",
    "\n",
    "# transform = transforms.Compose([\n",
    "#     transforms.RandomResizedCrop(128),\n",
    "#     transforms.ToTensor()\n",
    "# ])\n",
    "\n",
    "test_set = datasets.ImageFolder(\n",
    "    root=test_dir,\n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "test_set = WrapFewShotDataset(test_set)\n",
    "\n",
    "test_sampler = FewShotSampler(\n",
    "    test_set, n_way=n_way, n_shot=n_shot, n_query=n_query, n_tasks=n_test_tasks\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_sampler=test_sampler,\n",
    "    num_workers=n_workers,\n",
    "    pin_memory=True,\n",
    "    collate_fn=test_sampler.episodic_collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pr5aDnD-qnpJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Second step: we instantiate a few-shot classifier using our trained ResNet as backbone, and run it on the test data. We keep using Prototypical Networks for consistence, but at this point you could basically use any few-shot classifier that takes no additional trainable parameters.\n",
    "\n",
    "Like we did during validation, we need to tell our ResNet to not use its last fully connected layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lK6WOtpwqnpJ",
    "outputId": "89f92fcd-c8b5-4bda-d1b7-294ccfaf19c0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 573/1000 [53:19<39:44,  5.58s/it, accuracy=0.486]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/eileen/Projects/Pytorch-Projects/Few_shot_learning/classical_training.ipynb Cell 24\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/eileen/Projects/Pytorch-Projects/Few_shot_learning/classical_training.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m model\u001b[39m.\u001b[39mset_use_fc(\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/eileen/Projects/Pytorch-Projects/Few_shot_learning/classical_training.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m accuracy \u001b[39m=\u001b[39m evaluate(few_shot_classifier, test_loader, device\u001b[39m=\u001b[39;49mDEVICE)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/eileen/Projects/Pytorch-Projects/Few_shot_learning/classical_training.ipynb#X31sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mAverage accuracy : \u001b[39m\u001b[39m{\u001b[39;00m(\u001b[39m100\u001b[39m\u001b[39m \u001b[39m\u001b[39m*\u001b[39m\u001b[39m \u001b[39maccuracy)\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m %\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/Pytorch-Projects/Few_shot_learning/fewshot_utils.py:106\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(model, data_loader, device, use_tqdm, tqdm_prefix)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(\n\u001b[1;32m     94\u001b[0m     \u001b[39menumerate\u001b[39m(data_loader),\n\u001b[1;32m     95\u001b[0m     total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(data_loader),\n\u001b[1;32m     96\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m use_tqdm,\n\u001b[1;32m     97\u001b[0m     desc\u001b[39m=\u001b[39mtqdm_prefix,\n\u001b[1;32m     98\u001b[0m ) \u001b[39mas\u001b[39;00m tqdm_eval:\n\u001b[1;32m     99\u001b[0m     \u001b[39mfor\u001b[39;00m _, (\n\u001b[1;32m    100\u001b[0m         support_images,\n\u001b[1;32m    101\u001b[0m         support_labels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    104\u001b[0m         _,\n\u001b[1;32m    105\u001b[0m     ) \u001b[39min\u001b[39;00m tqdm_eval:\n\u001b[0;32m--> 106\u001b[0m         correct, total \u001b[39m=\u001b[39m evaluate_on_one_task(\n\u001b[1;32m    107\u001b[0m             model,\n\u001b[1;32m    108\u001b[0m             support_images\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m    109\u001b[0m             support_labels\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m    110\u001b[0m             query_images\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m    111\u001b[0m             query_labels\u001b[39m.\u001b[39;49mto(device),\n\u001b[1;32m    112\u001b[0m         )\n\u001b[1;32m    114\u001b[0m         total_predictions \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m total\n\u001b[1;32m    115\u001b[0m         correct_predictions \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m correct\n",
      "File \u001b[0;32m~/Projects/Pytorch-Projects/Few_shot_learning/fewshot_utils.py:57\u001b[0m, in \u001b[0;36mevaluate_on_one_task\u001b[0;34m(model, support_images, support_labels, query_images, query_labels)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mevaluate_on_one_task\u001b[39m(\n\u001b[1;32m     47\u001b[0m     model: FewShotClassifier,\n\u001b[1;32m     48\u001b[0m     support_images: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     51\u001b[0m     query_labels: Tensor,\n\u001b[1;32m     52\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\u001b[39mint\u001b[39m, \u001b[39mint\u001b[39m]:\n\u001b[1;32m     53\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m    Returns the number of correct predictions of query labels, and the total number of\u001b[39;00m\n\u001b[1;32m     55\u001b[0m \u001b[39m    predictions.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     model\u001b[39m.\u001b[39;49mprocess_support_set(support_images, support_labels)\n\u001b[1;32m     58\u001b[0m     predictions \u001b[39m=\u001b[39m model(query_images)\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mdata\n\u001b[1;32m     59\u001b[0m     number_of_correct_predictions \u001b[39m=\u001b[39m (\n\u001b[1;32m     60\u001b[0m         (torch\u001b[39m.\u001b[39mmax(predictions, \u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m query_labels)\u001b[39m.\u001b[39msum()\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     61\u001b[0m     )\n",
      "File \u001b[0;32m~/Projects/Pytorch-Projects/Few_shot_learning/modules/few_shot_classifier.py:74\u001b[0m, in \u001b[0;36mFewShotClassifier.process_support_set\u001b[0;34m(self, support_images, support_labels)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprocess_support_set\u001b[39m(\n\u001b[1;32m     63\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     64\u001b[0m     support_images: Tensor,\n\u001b[1;32m     65\u001b[0m     support_labels: Tensor,\n\u001b[1;32m     66\u001b[0m ):\n\u001b[1;32m     67\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39m    Harness information from the support set, so that query labels can later be predicted using a forward call.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[39m    The default behaviour shared by most few-shot classifiers is to compute prototypes and store the support set.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[39m        support_labels: labels of support set images of shape (n_support, )\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_prototypes_and_store_support_set(support_images, support_labels)\n",
      "File \u001b[0;32m~/Projects/Pytorch-Projects/Few_shot_learning/modules/few_shot_classifier.py:165\u001b[0m, in \u001b[0;36mFewShotClassifier.compute_prototypes_and_store_support_set\u001b[0;34m(self, support_images, support_labels)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39mExtract support features, compute prototypes, and store support labels, features, and prototypes.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m    support_images: images of the support set of shape (n_support, **image_shape)\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39m    support_labels: labels of support set images of shape (n_support, )\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_labels \u001b[39m=\u001b[39m support_labels\n\u001b[0;32m--> 165\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_features(support_images)\n\u001b[1;32m    166\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_error_if_features_are_multi_dimensional(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_features)\n\u001b[1;32m    167\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprototypes \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_prototypes(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msupport_features, support_labels)\n",
      "File \u001b[0;32m~/Projects/Pytorch-Projects/Few_shot_learning/modules/few_shot_classifier.py:90\u001b[0m, in \u001b[0;36mFewShotClassifier.compute_features\u001b[0;34m(self, images)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_features\u001b[39m(\u001b[39mself\u001b[39m, images: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     83\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[39m    Compute features from images and perform centering and normalization.\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39m        features of shape (n_images, feature_dimension)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m     original_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbackbone(images)\n\u001b[1;32m     91\u001b[0m     centered_features \u001b[39m=\u001b[39m original_features \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_centering\n\u001b[1;32m     92\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_normalization \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tiles/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tiles/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Projects/Pytorch-Projects/Few_shot_learning/modules/resnet.py:116\u001b[0m, in \u001b[0;36mResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m    107\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39m    Forward pass through the ResNet.\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m            otherwise of shape (batch_size, **feature_shape)\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m    115\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayer4(\n\u001b[0;32m--> 116\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer3(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer2(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlayer1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrelu(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn1(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x))))))\n\u001b[1;32m    117\u001b[0m     )\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39muse_pooling:\n\u001b[1;32m    120\u001b[0m         x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mflatten(\n\u001b[1;32m    121\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mavgpool(x),\n\u001b[1;32m    122\u001b[0m             \u001b[39m1\u001b[39m,\n\u001b[1;32m    123\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda3/envs/tiles/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tiles/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tiles/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/tiles/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tiles/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tiles/lib/python3.10/site-packages/torchvision/models/resnet.py:92\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m     90\u001b[0m     identity \u001b[39m=\u001b[39m x\n\u001b[0;32m---> 92\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconv1(x)\n\u001b[1;32m     93\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbn1(out)\n\u001b[1;32m     94\u001b[0m     out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/miniconda3/envs/tiles/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tiles/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tiles/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conv_forward(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "File \u001b[0;32m~/miniconda3/envs/tiles/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode \u001b[39m!=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mzeros\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39mconv2d(F\u001b[39m.\u001b[39mpad(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[39m0\u001b[39m), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdilation, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mconv2d(\u001b[39minput\u001b[39;49m, weight, bias, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstride,\n\u001b[1;32m    457\u001b[0m                 \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdilation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgroups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.set_use_fc(False)\n",
    "\n",
    "accuracy = evaluate(few_shot_classifier, test_loader, device=DEVICE)\n",
    "print(f\"Average accuracy : {(100 * accuracy):.2f} %\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'modules.resnet.ResNet'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "getting data:   0%|          | 0/1000 [00:00<?, ?it/s]\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f1dd787be20>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/eileen/miniconda3/envs/tiles/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1478, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/eileen/miniconda3/envs/tiles/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1442, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/eileen/miniconda3/envs/tiles/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/eileen/miniconda3/envs/tiles/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/home/eileen/miniconda3/envs/tiles/lib/python3.10/multiprocessing/connection.py\", line 931, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/eileen/miniconda3/envs/tiles/lib/python3.10/selectors.py\", line 416, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "import  numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "\n",
    "def eval_one_task(\n",
    "    support_images: torch.Tensor,\n",
    "    support_labels: torch.Tensor,\n",
    "    query_images: torch.Tensor,\n",
    "    query_labels: torch.Tensor,\n",
    "    class_ids: torch.Tensor\n",
    "):\n",
    "    \"\"\"\n",
    "    Returns the number of correct predictions of query labels, and the total number of predictions.\n",
    "    \"\"\"\n",
    "\n",
    "    few_shot_classifier.process_support_set(support_images, support_labels)\n",
    "\n",
    "    print(\"hi\")\n",
    "    print(type(few_shot_classifier))\n",
    "    predictions = few_shot_classifier(query_images).detach().data\n",
    "    print(type(predictions),predictions.shape)\n",
    "    \n",
    "    pred_labels = []\n",
    "    class_labels = []\n",
    "\n",
    "    for i, label in enumerate(query_labels):\n",
    "        pred_labels.append(class_ids[predictions[i]])\n",
    "        class_labels.append(class_ids[label])\n",
    "\n",
    "    return pred_labels, class_labels\n",
    "\n",
    "\n",
    "def get_data(data_loader: DataLoader):\n",
    "    # We'll count everything and compute the ratio at the end\n",
    "    pred = []\n",
    "    true = []\n",
    "\n",
    "    device = \"cpu\"\n",
    "\n",
    "    # eval mode affects the behaviour of some layers (such as batch normalization or dropout)\n",
    "    # no_grad() tells torch not to keep in memory the whole computational graph (it's more lightweight this way)\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with tqdm(\n",
    "            enumerate(data_loader),\n",
    "            total=len(data_loader),\n",
    "            desc=\"getting data\",\n",
    "        ) as tqdm_eval:\n",
    "            for _, (\n",
    "                support_images,\n",
    "                support_labels,\n",
    "                query_images,\n",
    "                query_labels,\n",
    "                _,\n",
    "            ) in tqdm_eval:\n",
    "                correct, total = eval_one_task(\n",
    "                    model,\n",
    "                    support_images.to(device),\n",
    "                    support_labels.to(device),\n",
    "                    query_images.to(device),\n",
    "                    query_labels.to(device),\n",
    "                )\n",
    "\n",
    "        # for (\n",
    "        #     support_images,\n",
    "        #     support_labels,\n",
    "        #     query_images,\n",
    "        #     query_labels,\n",
    "        #     class_ids,\n",
    "        # ) in tqdm(data_loader, total=len(data_loader)):\n",
    "\n",
    "        #     correct, total = get_labels(\n",
    "        #         query_images, query_labels, class_ids\n",
    "        #     )\n",
    "\n",
    "            pred.extend(correct)\n",
    "            true.extend(total)\n",
    "\n",
    "    return pred, true\n",
    "\n",
    "# generate confusion matrix\n",
    "classes = ('buildings', 'chaparral', 'denseresidential', 'intersection', 'mediumresidential',\n",
    "    'mobilehomepark', 'sparseresidential', 'storagetanks', 'tenniscourt')\n",
    "\n",
    "pred, true = get_data(test_loader)\n",
    "\n",
    "cf_matrix = confusion_matrix(true,pred)\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1)[:, None], index = [i for i in classes],\n",
    "                    columns = [i for i in classes])\n",
    "\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)\n",
    "plt.savefig('proto.png')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "id": "qe5fBbm4qnpJ",
    "pycharm": {
     "name": "#%% raw\n"
    }
   },
   "source": [
    "Congrats! You trained a network with cross entropy and used it with a few-shot learning method at test time."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "classical_training.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
