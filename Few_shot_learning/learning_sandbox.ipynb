{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 4.5382],\n",
      "        [7.2432, 3.8322],\n",
      "        [4.5579, 0.3791]])\n"
     ]
    }
   ],
   "source": [
    "# Learning pytorch cdist\n",
    "a = torch.tensor([[3, 4], [-0.3108, -2.4423], [-0.4821,  1.059]])\n",
    "b = torch.tensor([[3, 4], [-0.6986,  1.3702]])\n",
    "dists = torch.cdist(a, b, p=2) # p=2 means norm 2, or euclidian distance\n",
    "print(dists)\n",
    "\n",
    "# row i of dists is the distances of element i in a\n",
    "# col j of dists is the distances of element i in a compared to element j of b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototypical Network\n",
    "\n",
    "## Sizes in original version:\n",
    "\n",
    "support_features: [25, 512]\n",
    "\n",
    "query_features: [25, 512]\n",
    "\n",
    "prototypes: [5, 512]\n",
    "\n",
    "dists: [50, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support: tensor([[0.0747, 0.8336, 0.3018, 0.1586, 0.0862, 0.6559, 0.0182, 0.6178, 0.2983,\n",
      "         0.6495],\n",
      "        [0.9465, 0.0946, 0.6247, 0.2405, 0.1311, 0.0223, 0.2917, 0.3001, 0.5003,\n",
      "         0.0394],\n",
      "        [0.0486, 0.1500, 0.6379, 0.0133, 0.7661, 0.4694, 0.5304, 0.4993, 0.2591,\n",
      "         0.3628]])\n",
      "prototypes: tensor([[0.0747, 0.8336, 0.3018, 0.1586, 0.0862, 0.6559, 0.0182, 0.6178, 0.2983,\n",
      "         0.6495],\n",
      "        [0.9465, 0.0946, 0.6247, 0.2405, 0.1311, 0.0223, 0.2917, 0.3001, 0.5003,\n",
      "         0.0394],\n",
      "        [0.0486, 0.1500, 0.6379, 0.0133, 0.7661, 0.4694, 0.5304, 0.4993, 0.2591,\n",
      "         0.3628]])\n"
     ]
    }
   ],
   "source": [
    "# Prototypical Network\n",
    "support_features = torch.rand((3,10))\n",
    "query_features = torch.rand((5,10))\n",
    "\n",
    "print(\"support:\", support_features)\n",
    "\n",
    "n_way = 3\n",
    "\n",
    "support_labels = torch.tensor([0,1,2])\n",
    "\n",
    "# Prototype i is the mean of all instances of features corresponding to labels == i\n",
    "prototypes = torch.cat([support_features[torch.nonzero(support_labels==i)].mean(0) \n",
    "                        for i in range(n_way)])\n",
    "\n",
    "print(\"prototypes:\",prototypes)\n",
    "\n",
    "dists = torch.cdist(query_features, prototypes)\n",
    "# print(dists)\n",
    "\n",
    "scores = -dists\n",
    "# print(scores)\n",
    "\n",
    "# scores are negative of dists because the largest (least negative)\n",
    "# score value is the answer, and the smallest distance is the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset (wrapper) Class\n",
    "\n",
    "Wrap a dataset in a FewShotDataset.\n",
    "\n",
    "Args:\n",
    "\n",
    "dataset: dataset to wrap\n",
    "\n",
    "image_position_in_get_item_output: position of the image in the tuple returned\n",
    "    by dataset.__getitem__(). Default: 0\n",
    "    \n",
    "label_position_in_get_item_output: position of the label in the tuple returned\n",
    "    by dataset.__getitem__(). Default: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Iterator\n",
    "import torch\n",
    "import random\n",
    "from torch import Tensor\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from few_shot_dataset import FewShotDataset\n",
    "from pathlib import Path\n",
    "\n",
    "class Dummy_dataset(FewShotDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        image_position_in_get_item_output: int = 0,\n",
    "        label_position_in_get_item_output: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Wrap a dataset in a FewShotDataset.\n",
    "        Args:\n",
    "            dataset: dataset to wrap\n",
    "            image_position_in_get_item_output: position of the image in the tuple returned\n",
    "                by dataset.__getitem__(). Default: 0\n",
    "            label_position_in_get_item_output: position of the label in the tuple returned\n",
    "                by dataset.__getitem__(). Default: 1\n",
    "        \"\"\"\n",
    "\n",
    "        self.source_dataset = dataset\n",
    "        self.image_position_in_get_item_output = image_position_in_get_item_output\n",
    "        self.label_position_in_get_item_output = label_position_in_get_item_output\n",
    "        self.n_shot = 5\n",
    "        self.n_way = 5\n",
    "        self.n_query = 10\n",
    "        self.n_tasks = 100\n",
    "\n",
    "        self.labels = []\n",
    "\n",
    "        for [_,label] in dataset:\n",
    "            self.labels.append(label)\n",
    "\n",
    "\n",
    "    def __getitem__(self, item: int) -> Tuple[Tensor, int]:\n",
    "        return (\n",
    "            self.source_dataset[item][self.image_position_in_get_item_output],\n",
    "            self.source_dataset[item][self.label_position_in_get_item_output],\n",
    "        )\n",
    "\n",
    "    def get_labels(self) -> List[int]:\n",
    "        return self.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Optional, Sized, Dict\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "class DummySampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset, n_way, n_shot, n_query, n_tasks, data_source: Sized | None = None) -> None:\n",
    "        super().__init__(data_source)\n",
    "        self.n_way = n_way # number of classes in one task\n",
    "        self.n_shot = n_shot # number of support images for each class in one task\n",
    "        self.n_query = n_query # number of query images for each class in one task\n",
    "        self.n_tasks = n_tasks # number of tasks to sample (each bundle of support and query)\n",
    "\n",
    "        self.items_per_label: Dict[int,List[int]] = {}\n",
    "\n",
    "        for item, label in enumerate(dataset.get_labels()):\n",
    "            if label not in self.items_per_label:\n",
    "                self.items_per_label[label] = [item]\n",
    "            else:\n",
    "                self.items_per_label[label].append(item)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_tasks\n",
    "    \n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        yield torch.cat([\n",
    "            torch.tensor(\n",
    "                    random.sample(\n",
    "                        self.items_per_label[label], self.n_shot + self.n_query # getting n_shot + n_query random images from selected label\n",
    "                    )\n",
    "                )\n",
    "                for label in random.sample(\n",
    "                    sorted(self.items_per_label.keys()), self.n_way # samples n_way classes\n",
    "                )\n",
    "        ]).tolist() # returns a list of n_way*(n_shot+n_query) numbers representing image indexes within entire dataset\n",
    "\n",
    "    def episodic_collate_fn(\n",
    "        self, input_data: List[Tuple[Tensor, Union[Tensor, int]]]\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, List[int]]:\n",
    "        \n",
    "        # cast label of each data image to an integer\n",
    "        input_data_with_int_labels = self._cast_input_data_to_tensor_int_tuple(input_data)\n",
    "\n",
    "        # true_class_ids is a list from [0-max_class_label] ex: [0,1,2,3,4,5]\n",
    "        true_class_ids = list({x[1] for x in input_data_with_int_labels})\n",
    "\n",
    "        # unsqueeze adds additional dimension, so go from torch.Size([3, 28, 28]) to torch.Size([1, 3, 28, 28]) for each image\n",
    "        # this way, the concatenated result dimensions are torch.Size([420, 3, 28, 28]) rather than torch.Size([1260, 28, 28])\n",
    "        all_images = torch.cat([x[0].unsqueeze(0) for x in input_data_with_int_labels])\n",
    "\n",
    "        # reshape(*var) unpacks the dimensions of var, so reshape(*all_images.shape[1:]) where each image is size (1,2,3) is the same as reshape(1,2,3)\n",
    "        # this code puts each image in a matrix where each row is a different class (n_way) with each row containing n_shot+n_query images from the class\n",
    "        all_images = all_images.reshape(\n",
    "            (self.n_way, self.n_shot + self.n_query, *all_images.shape[1:])\n",
    "        )\n",
    "\n",
    "        # puts list of labels into matrix form matching dimensions above matrix\n",
    "        # ex: [2,2,1,1] => [[2,2],[1,1]] (all rows are the same number/class)\n",
    "        all_labels = torch.tensor(\n",
    "            [true_class_ids.index(x[1]) for x in input_data_with_int_labels]\n",
    "        ).reshape((self.n_way, self.n_shot + self.n_query))\n",
    "\n",
    "        # take the first n_shot images and combine them into a group (support)\n",
    "        support_images = all_images[:, : self.n_shot].reshape(\n",
    "            (-1, *all_images.shape[2:])\n",
    "        )\n",
    "\n",
    "        # take the rest of the images and combine them into a group (query)\n",
    "        query_images = all_images[:, self.n_shot :].reshape((-1, *all_images.shape[2:]))\n",
    "\n",
    "        # group the support and query labels and flatten so they are a list\n",
    "        # classes in the rows are the same between them, the difference is the length of each row (but the rows go away since they are flattened)\n",
    "        support_labels = all_labels[:, : self.n_shot].flatten()\n",
    "        query_labels = all_labels[:, self.n_shot :].flatten()\n",
    "        \n",
    "        return (\n",
    "            support_images,\n",
    "            support_labels,\n",
    "            query_images,\n",
    "            query_labels,\n",
    "            true_class_ids,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _cast_input_data_to_tensor_int_tuple(\n",
    "        input_data: List[Tuple[Tensor, Union[Tensor, int]]]\n",
    "    ) -> List[Tuple[Tensor, int]]:\n",
    "        return [(image, int(label)) for (image, label) in input_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/UCMerced-Test directory exists.\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "\n",
    "# Setup path to data folder\n",
    "data_path = Path(\"data\")\n",
    "image_path = data_path / \"UCMerced-Test\"\n",
    "\n",
    "# Check if image folder exists\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory\")\n",
    "    exit()\n",
    "\n",
    "# Setup train and testing paths\n",
    "test_dir = image_path / \"Test\"\n",
    "\n",
    "# Write transform for image\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize([int(image_size * 1.15), int(image_size * 1.15)]),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_set = datasets.ImageFolder(\n",
    "    root=test_dir,\n",
    "    transform=test_transform,\n",
    ")\n",
    "\n",
    "test_set = Dummy_dataset(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episodic Collate Function\n",
    "\n",
    "Collate function to be used as argument for the collate_fn parameter of episodic data loaders.\n",
    "\n",
    "Args:\n",
    "\n",
    "input_data: each element is a tuple containing:\n",
    "- an image as a torch Tensor of shape (n_channels, height, width)\n",
    "- the label of this image as an int or a 0-dim tensor\n",
    "        \n",
    "Returns:\n",
    "\n",
    "tuple(Tensor, Tensor, Tensor, Tensor, list[int]): respectively:\n",
    "- support images of shape (n_way * n_shot, n_channels, height, width),\n",
    "- their labels of shape (n_way * n_shot),\n",
    "- query images of shape (n_way * n_query, n_channels, height, width)\n",
    "- their labels of shape (n_way * n_query),\n",
    "- the dataset class ids of the class sampled in the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_sampler = DummySampler(\n",
    "    test_set, n_way=5, n_shot=5, n_query=10, n_tasks=100\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_sampler=test_sampler,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=test_sampler.episodic_collate_fn,\n",
    ")\n",
    "\n",
    "(\n",
    "    example_support_images,\n",
    "    example_support_labels,\n",
    "    example_query_images,\n",
    "    example_query_labels,\n",
    "    example_class_ids,\n",
    ") = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/eileen/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/eileen/anaconda3/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4, 4,\n",
      "        4, 4, 4, 4, 4, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 3, 3, 3, 3, 3, 3, 3,\n",
      "        3, 3])\n",
      "tensor([3, 2, 2, 2, 4, 2, 3, 2, 2, 2, 0, 4, 0, 4, 0, 0, 0, 0, 4, 0, 4, 4, 0, 4,\n",
      "        4, 0, 0, 1, 4, 3, 3, 0, 1, 4, 1, 2, 1, 1, 0, 1, 4, 3, 3, 0, 0, 2, 2, 0,\n",
      "        4, 2])\n",
      "tensor([False,  True,  True,  True, False,  True, False,  True,  True,  True,\n",
      "         True, False,  True, False,  True,  True,  True,  True, False,  True,\n",
      "         True,  True, False,  True,  True, False, False, False,  True, False,\n",
      "        False, False,  True, False,  True, False,  True,  True, False,  True,\n",
      "        False,  True,  True, False, False, False, False, False, False, False])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'int'>\n",
      "(26, 50)\n"
     ]
    }
   ],
   "source": [
    "from src.Prototypical_networks import PrototypicalNetworks\n",
    "from torchvision.models import resnet18\n",
    "from torch import nn\n",
    "\n",
    "convolutional_network = resnet18(pretrained=True)\n",
    "convolutional_network.fc = nn.Flatten()\n",
    "\n",
    "model = PrototypicalNetworks(convolutional_network)\n",
    "\n",
    "def evaluate_on_one_task(\n",
    "    support_images: torch.Tensor,\n",
    "    support_labels: torch.Tensor,\n",
    "    query_images: torch.Tensor,\n",
    "    query_labels: torch.Tensor,\n",
    ") -> [int, int]:\n",
    "    \"\"\"\n",
    "    Returns the number of correct predictions of query labels, and the total number of predictions.\n",
    "    \"\"\"\n",
    "    print(query_labels)\n",
    "    print(torch.max(model(support_images, support_labels, query_images).detach().data,1)[1])\n",
    "    print(torch.max(model(support_images, support_labels, query_images).detach().data,1)[1]==query_labels)\n",
    "    print(type((torch.max(model(support_images, support_labels, query_images).detach().data,1)[1]==query_labels).sum()))\n",
    "    print(type((torch.max(model(support_images, support_labels, query_images).detach().data,1)[1]==query_labels).sum().item()))\n",
    "\n",
    "    return (\n",
    "        torch.max(\n",
    "            model(support_images, support_labels, query_images)\n",
    "            .detach()\n",
    "            .data,\n",
    "            1,\n",
    "        )[1] # this returns a tensor/list of indexes (labels) of highest scores across each row in matrix\n",
    "        == query_labels # this compares tensor with real labels to tensor of predicted labels and returns a tensor of True and False\n",
    "    ).sum().item(), len(query_labels)\n",
    "    # sum adds up number of \"True\"s, item turns it from tensor to int\n",
    "\n",
    "print(evaluate_on_one_task(\n",
    "    example_support_images,\n",
    "    example_support_labels,\n",
    "    example_query_images,\n",
    "    example_query_labels\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
