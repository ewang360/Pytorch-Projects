{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 4.5382],\n",
      "        [7.2432, 3.8322],\n",
      "        [4.5579, 0.3791]])\n"
     ]
    }
   ],
   "source": [
    "# Learning pytorch cdist\n",
    "a = torch.tensor([[3, 4], [-0.3108, -2.4423], [-0.4821,  1.059]])\n",
    "b = torch.tensor([[3, 4], [-0.6986,  1.3702]])\n",
    "dists = torch.cdist(a, b, p=2) # p=2 means norm 2, or euclidian distance\n",
    "print(dists)\n",
    "\n",
    "# row i of dists is the distances of element i in a\n",
    "# col j of dists is the distances of element i in a compared to element j of b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototypical Network\n",
    "\n",
    "## Sizes in original version:\n",
    "\n",
    "support_features: [25, 512]\n",
    "\n",
    "query_features: [25, 512]\n",
    "\n",
    "prototypes: [5, 512]\n",
    "\n",
    "dists: [50, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support: tensor([[0.7511, 0.7107, 0.5242, 0.4270, 0.9622, 0.1226, 0.5837, 0.5515, 0.6753,\n",
      "         0.2185],\n",
      "        [0.5593, 0.3586, 0.8593, 0.5987, 0.7437, 0.3188, 0.4466, 0.5877, 0.4988,\n",
      "         0.9259],\n",
      "        [0.1332, 0.8960, 0.2055, 0.8800, 0.7347, 0.6509, 0.8275, 0.7082, 0.2440,\n",
      "         0.2309]])\n",
      "prototypes: tensor([[0.7511, 0.7107, 0.5242, 0.4270, 0.9622, 0.1226, 0.5837, 0.5515, 0.6753,\n",
      "         0.2185],\n",
      "        [0.5593, 0.3586, 0.8593, 0.5987, 0.7437, 0.3188, 0.4466, 0.5877, 0.4988,\n",
      "         0.9259],\n",
      "        [0.1332, 0.8960, 0.2055, 0.8800, 0.7347, 0.6509, 0.8275, 0.7082, 0.2440,\n",
      "         0.2309]])\n",
      "tensor([[1.0895, 1.0045, 1.0592],\n",
      "        [1.0871, 0.8147, 1.7394],\n",
      "        [1.3743, 1.2679, 1.2579],\n",
      "        [1.1231, 1.0116, 1.3322],\n",
      "        [1.2640, 1.1765, 1.3550]])\n",
      "tensor([[-1.0895, -1.0045, -1.0592],\n",
      "        [-1.0871, -0.8147, -1.7394],\n",
      "        [-1.3743, -1.2679, -1.2579],\n",
      "        [-1.1231, -1.0116, -1.3322],\n",
      "        [-1.2640, -1.1765, -1.3550]])\n"
     ]
    }
   ],
   "source": [
    "# Prototypical Network\n",
    "support_features = torch.rand((3,10))\n",
    "query_features = torch.rand((5,10))\n",
    "\n",
    "print(\"support:\", support_features)\n",
    "\n",
    "n_way = 3\n",
    "\n",
    "support_labels = torch.tensor([0,1,2])\n",
    "\n",
    "# Prototype i is the mean of all instances of features corresponding to labels == i\n",
    "prototypes = torch.cat([support_features[torch.nonzero(support_labels==i)].mean(0) \n",
    "                        for i in range(n_way)])\n",
    "\n",
    "print(\"prototypes:\",prototypes)\n",
    "\n",
    "dists = torch.cdist(query_features, prototypes)\n",
    "# print(dists)\n",
    "\n",
    "scores = -dists\n",
    "# print(scores)\n",
    "\n",
    "# scores are negative of dists because the largest (least negative)\n",
    "# score value is the answer, and the smallest distance is the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset (wrapper) Class\n",
    "\n",
    "Wrap a dataset in a FewShotDataset.\n",
    "\n",
    "Args:\n",
    "\n",
    "dataset: dataset to wrap\n",
    "\n",
    "image_position_in_get_item_output: position of the image in the tuple returned\n",
    "    by dataset.__getitem__(). Default: 0\n",
    "    \n",
    "label_position_in_get_item_output: position of the label in the tuple returned\n",
    "    by dataset.__getitem__(). Default: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Iterator\n",
    "import torch\n",
    "import random\n",
    "from torch import Tensor\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from few_shot_dataset import FewShotDataset\n",
    "from pathlib import Path\n",
    "\n",
    "class Dummy_dataset(FewShotDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        image_position_in_get_item_output: int = 0,\n",
    "        label_position_in_get_item_output: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Wrap a dataset in a FewShotDataset.\n",
    "        Args:\n",
    "            dataset: dataset to wrap\n",
    "            image_position_in_get_item_output: position of the image in the tuple returned\n",
    "                by dataset.__getitem__(). Default: 0\n",
    "            label_position_in_get_item_output: position of the label in the tuple returned\n",
    "                by dataset.__getitem__(). Default: 1\n",
    "        \"\"\"\n",
    "\n",
    "        self.source_dataset = dataset\n",
    "        self.image_position_in_get_item_output = image_position_in_get_item_output\n",
    "        self.label_position_in_get_item_output = label_position_in_get_item_output\n",
    "        self.n_shot = 5\n",
    "        self.n_way = 5\n",
    "        self.n_query = 10\n",
    "        self.n_tasks = 100\n",
    "\n",
    "        self.labels = []\n",
    "\n",
    "        for [_,label] in dataset:\n",
    "            self.labels.append(label)\n",
    "\n",
    "\n",
    "    def __getitem__(self, item: int) -> Tuple[Tensor, int]:\n",
    "        return (\n",
    "            self.source_dataset[item][self.image_position_in_get_item_output],\n",
    "            self.source_dataset[item][self.label_position_in_get_item_output],\n",
    "        )\n",
    "\n",
    "    def get_labels(self) -> List[int]:\n",
    "        return self.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Optional, Sized, Dict\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "class DummySampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset, n_way, n_shot, n_query, n_tasks, data_source: Sized | None = None) -> None:\n",
    "        super().__init__(data_source)\n",
    "        self.n_way = n_way # number of classes in one task\n",
    "        self.n_shot = n_shot # number of support images for each class in one task\n",
    "        self.n_query = n_query # number of query images for each class in one task\n",
    "        self.n_tasks = n_tasks # number of tasks to sample (each bundle of support and query)\n",
    "\n",
    "        self.items_per_label: Dict[int,List[int]] = {}\n",
    "\n",
    "        for item, label in enumerate(dataset.get_labels()):\n",
    "            if label not in self.items_per_label:\n",
    "                self.items_per_label[label] = [item]\n",
    "            else:\n",
    "                self.items_per_label[label].append(item)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_tasks\n",
    "    \n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        yield torch.cat([\n",
    "            torch.tensor(\n",
    "                    random.sample(\n",
    "                        self.items_per_label[label], self.n_shot + self.n_query # getting n_shot + n_query random images from selected label\n",
    "                    )\n",
    "                )\n",
    "                for label in random.sample(\n",
    "                    sorted(self.items_per_label.keys()), self.n_way # samples n_way classes\n",
    "                )\n",
    "        ]).tolist() # returns a list of n_way*(n_shot+n_query) numbers representing image indexes within entire dataset\n",
    "\n",
    "    def episodic_collate_fn(\n",
    "        self, input_data: List[Tuple[Tensor, Union[Tensor, int]]]\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, List[int]]:\n",
    "        \n",
    "        # cast label of each data image to an integer\n",
    "        input_data_with_int_labels = self._cast_input_data_to_tensor_int_tuple(input_data)\n",
    "\n",
    "        # true_class_ids is a list from [0-max_class_label] ex: [0,1,2,3,4,5]\n",
    "        true_class_ids = list({x[1] for x in input_data_with_int_labels})\n",
    "\n",
    "        # unsqueeze adds additional dimension, so go from torch.Size([3, 28, 28]) to torch.Size([1, 3, 28, 28]) for each image\n",
    "        # this way, the concatenated result dimensions are torch.Size([420, 3, 28, 28]) rather than torch.Size([1260, 28, 28])\n",
    "        all_images = torch.cat([x[0].unsqueeze(0) for x in input_data_with_int_labels])\n",
    "\n",
    "        # reshape(*var) unpacks the dimensions of var, so reshape(*all_images.shape[1:]) where each image is size (1,2,3) is the same as reshape(1,2,3)\n",
    "        # this code puts each image in a matrix where each row is a different class (n_way) with each row containing n_shot+n_query images from the class\n",
    "        all_images = all_images.reshape(\n",
    "            (self.n_way, self.n_shot + self.n_query, *all_images.shape[1:])\n",
    "        )\n",
    "\n",
    "        # puts list of labels into matrix form matching dimensions above matrix\n",
    "        # ex: [2,2,1,1] => [[2,2],[1,1]] (all rows are the same number/class)\n",
    "        all_labels = torch.tensor(\n",
    "            [true_class_ids.index(x[1]) for x in input_data_with_int_labels]\n",
    "        ).reshape((self.n_way, self.n_shot + self.n_query))\n",
    "\n",
    "        # take the first n_shot images and combine them into a group (support)\n",
    "        support_images = all_images[:, : self.n_shot].reshape(\n",
    "            (-1, *all_images.shape[2:])\n",
    "        )\n",
    "\n",
    "        # take the rest of the images and combine them into a group (query)\n",
    "        query_images = all_images[:, self.n_shot :].reshape((-1, *all_images.shape[2:]))\n",
    "\n",
    "        # group the support and query labels and flatten so they are a list\n",
    "        # classes in the rows are the same between them, the difference is the length of each row (but the rows go away since they are flattened)\n",
    "        support_labels = all_labels[:, : self.n_shot].flatten()\n",
    "        query_labels = all_labels[:, self.n_shot :].flatten()\n",
    "        \n",
    "        return (\n",
    "            support_images,\n",
    "            support_labels,\n",
    "            query_images,\n",
    "            query_labels,\n",
    "            true_class_ids,\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def _cast_input_data_to_tensor_int_tuple(\n",
    "        input_data: List[Tuple[Tensor, Union[Tensor, int]]]\n",
    "    ) -> List[Tuple[Tensor, int]]:\n",
    "        return [(image, int(label)) for (image, label) in input_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/UCMerced-Test directory exists.\n",
      "[228, 221, 233, 226, 234, 235, 229, 238, 232, 230, 220, 227, 222, 223, 225, 168, 166, 160, 170, 165, 171, 164, 162, 161, 167, 173, 176, 175, 179, 177, 328, 334, 337, 331, 332, 339, 330, 326, 327, 320, 336, 329, 335, 325, 338, 308, 306, 307, 304, 316, 313, 318, 305, 300, 301, 303, 312, 309, 311, 314, 213, 204, 216, 201, 205, 208, 207, 214, 219, 206, 218, 200, 217, 211, 212]\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "\n",
    "# Setup path to data folder\n",
    "data_path = Path(\"data\")\n",
    "image_path = data_path / \"UCMerced-Test\"\n",
    "\n",
    "# Check if image folder exists\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory\")\n",
    "    exit()\n",
    "\n",
    "# Setup train and testing paths\n",
    "test_dir = image_path / \"Test\"\n",
    "\n",
    "# Write transform for image\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize([int(image_size * 1.15), int(image_size * 1.15)]),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_set = datasets.ImageFolder(\n",
    "    root=test_dir,\n",
    "    transform=test_transform,\n",
    ")\n",
    "\n",
    "test_set = Dummy_dataset(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episodic Collate Function\n",
    "\n",
    "Collate function to be used as argument for the collate_fn parameter of episodic data loaders.\n",
    "\n",
    "Args:\n",
    "\n",
    "input_data: each element is a tuple containing:\n",
    "- an image as a torch Tensor of shape (n_channels, height, width)\n",
    "- the label of this image as an int or a 0-dim tensor\n",
    "        \n",
    "Returns:\n",
    "\n",
    "tuple(Tensor, Tensor, Tensor, Tensor, list[int]): respectively:\n",
    "- support images of shape (n_way * n_shot, n_channels, height, width),\n",
    "- their labels of shape (n_way * n_shot),\n",
    "- query images of shape (n_way * n_query, n_channels, height, width)\n",
    "- their labels of shape (n_way * n_query),\n",
    "- the dataset class ids of the class sampled in the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 0, 0, 0, 0,\n",
      "        0])\n",
      "tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_sampler = DummySampler(\n",
    "    test_set, n_way=5, n_shot=5, n_query=10, n_tasks=100\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_set,\n",
    "    batch_sampler=test_sampler,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    "    collate_fn=test_sampler.episodic_collate_fn,\n",
    ")\n",
    "\n",
    "(\n",
    "    example_support_images,\n",
    "    example_support_labels,\n",
    "    example_query_images,\n",
    "    example_query_labels,\n",
    "    example_class_ids,\n",
    ") = next(iter(test_loader))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
