{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 4.5382],\n",
      "        [7.2432, 3.8322],\n",
      "        [4.5579, 0.3791]])\n"
     ]
    }
   ],
   "source": [
    "# Learning pytorch cdist\n",
    "a = torch.tensor([[3, 4], [-0.3108, -2.4423], [-0.4821,  1.059]])\n",
    "b = torch.tensor([[3, 4], [-0.6986,  1.3702]])\n",
    "dists = torch.cdist(a, b, p=2) # p=2 means norm 2, or euclidian distance\n",
    "print(dists)\n",
    "\n",
    "# row i of dists is the distances of element i in a\n",
    "# col j of dists is the distances of element i in a compared to element j of b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prototypical Network\n",
    "\n",
    "## Sizes in original version:\n",
    "\n",
    "support_features: [25, 512]\n",
    "\n",
    "query_features: [25, 512]\n",
    "\n",
    "prototypes: [5, 512]\n",
    "\n",
    "dists: [50, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "support: tensor([[0.7511, 0.7107, 0.5242, 0.4270, 0.9622, 0.1226, 0.5837, 0.5515, 0.6753,\n",
      "         0.2185],\n",
      "        [0.5593, 0.3586, 0.8593, 0.5987, 0.7437, 0.3188, 0.4466, 0.5877, 0.4988,\n",
      "         0.9259],\n",
      "        [0.1332, 0.8960, 0.2055, 0.8800, 0.7347, 0.6509, 0.8275, 0.7082, 0.2440,\n",
      "         0.2309]])\n",
      "prototypes: tensor([[0.7511, 0.7107, 0.5242, 0.4270, 0.9622, 0.1226, 0.5837, 0.5515, 0.6753,\n",
      "         0.2185],\n",
      "        [0.5593, 0.3586, 0.8593, 0.5987, 0.7437, 0.3188, 0.4466, 0.5877, 0.4988,\n",
      "         0.9259],\n",
      "        [0.1332, 0.8960, 0.2055, 0.8800, 0.7347, 0.6509, 0.8275, 0.7082, 0.2440,\n",
      "         0.2309]])\n",
      "tensor([[1.0895, 1.0045, 1.0592],\n",
      "        [1.0871, 0.8147, 1.7394],\n",
      "        [1.3743, 1.2679, 1.2579],\n",
      "        [1.1231, 1.0116, 1.3322],\n",
      "        [1.2640, 1.1765, 1.3550]])\n",
      "tensor([[-1.0895, -1.0045, -1.0592],\n",
      "        [-1.0871, -0.8147, -1.7394],\n",
      "        [-1.3743, -1.2679, -1.2579],\n",
      "        [-1.1231, -1.0116, -1.3322],\n",
      "        [-1.2640, -1.1765, -1.3550]])\n"
     ]
    }
   ],
   "source": [
    "# Prototypical Network\n",
    "support_features = torch.rand((3,10))\n",
    "query_features = torch.rand((5,10))\n",
    "\n",
    "print(\"support:\", support_features)\n",
    "\n",
    "n_way = 3\n",
    "\n",
    "support_labels = torch.tensor([0,1,2])\n",
    "\n",
    "# Prototype i is the mean of all instances of features corresponding to labels == i\n",
    "prototypes = torch.cat([support_features[torch.nonzero(support_labels==i)].mean(0) \n",
    "                        for i in range(n_way)])\n",
    "\n",
    "print(\"prototypes:\",prototypes)\n",
    "\n",
    "dists = torch.cdist(query_features, prototypes)\n",
    "# print(dists)\n",
    "\n",
    "scores = -dists\n",
    "# print(scores)\n",
    "\n",
    "# scores are negative of dists because the largest (least negative)\n",
    "# score value is the answer, and the smallest distance is the answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset (wrapper) Class\n",
    "\n",
    "Wrap a dataset in a FewShotDataset.\n",
    "\n",
    "Args:\n",
    "\n",
    "dataset: dataset to wrap\n",
    "\n",
    "image_position_in_get_item_output: position of the image in the tuple returned\n",
    "    by dataset.__getitem__(). Default: 0\n",
    "    \n",
    "label_position_in_get_item_output: position of the label in the tuple returned\n",
    "    by dataset.__getitem__(). Default: 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List, Union, Iterator\n",
    "import torch\n",
    "import random\n",
    "from torch import Tensor\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset\n",
    "from few_shot_dataset import FewShotDataset\n",
    "from pathlib import Path\n",
    "\n",
    "class Dummy_dataset(FewShotDataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dataset: Dataset,\n",
    "        image_position_in_get_item_output: int = 0,\n",
    "        label_position_in_get_item_output: int = 1,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Wrap a dataset in a FewShotDataset.\n",
    "        Args:\n",
    "            dataset: dataset to wrap\n",
    "            image_position_in_get_item_output: position of the image in the tuple returned\n",
    "                by dataset.__getitem__(). Default: 0\n",
    "            label_position_in_get_item_output: position of the label in the tuple returned\n",
    "                by dataset.__getitem__(). Default: 1\n",
    "        \"\"\"\n",
    "\n",
    "        self.source_dataset = dataset\n",
    "        self.image_position_in_get_item_output = image_position_in_get_item_output\n",
    "        self.label_position_in_get_item_output = label_position_in_get_item_output\n",
    "        self.n_shot = 5\n",
    "        self.n_way = 5\n",
    "        self.n_query = 10\n",
    "        self.n_tasks = 100\n",
    "\n",
    "        self.labels = []\n",
    "\n",
    "        for [_,label] in dataset:\n",
    "            self.labels.append(label)\n",
    "\n",
    "\n",
    "    def __getitem__(self, item: int) -> Tuple[Tensor, int]:\n",
    "        return (\n",
    "            self.source_dataset[item][self.image_position_in_get_item_output],\n",
    "            self.source_dataset[item][self.label_position_in_get_item_output],\n",
    "        )\n",
    "    \n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        \"\"\"\n",
    "        Sample n_way labels uniformly at random,\n",
    "        and then sample n_shot + n_query items for each label, also uniformly at random.\n",
    "        Yields:\n",
    "            a list of indices of length (n_way * (n_shot + n_query))\n",
    "        \"\"\"\n",
    "        for _ in range(self.n_tasks):\n",
    "            yield torch.cat(\n",
    "                [\n",
    "                    torch.tensor(\n",
    "                        random.sample(\n",
    "                            self.items_per_label[label], self.n_shot + self.n_query\n",
    "                        )\n",
    "                    )\n",
    "                    for label in random.sample(\n",
    "                        sorted(self.items_per_label.keys()), self.n_way\n",
    "                    )\n",
    "                ]\n",
    "            ).tolist()\n",
    "    \n",
    "    def episodic_collate_fn(\n",
    "        self, input_data: List[Tuple[Tensor, Union[Tensor, int]]]\n",
    "    ) -> Tuple[Tensor, Tensor, Tensor, Tensor, List[int]]:\n",
    "        \n",
    "        # cast label of each data image to an integer\n",
    "        input_data_with_int_labels = self._cast_input_data_to_tensor_int_tuple(input_data)\n",
    "\n",
    "        # true_class_ids is a list from [0-max_class_label] ex: [0,1,2,3,4,5]\n",
    "        true_class_ids = list({x[1] for x in input_data_with_int_labels})\n",
    "\n",
    "        # unsqueeze adds additional dimension, so go from torch.Size([3, 28, 28]) to torch.Size([1, 3, 28, 28]) for each image\n",
    "        # this way, the end result dimensions are torch.Size([420, 3, 28, 28]) rather than torch.Size([1260, 28, 28])\n",
    "        all_images = torch.cat([x[0].unsqueeze(0) for x in input_data_with_int_labels])\n",
    "        print(all_images.reshape(\n",
    "            (self.n_way, self.n_shot + self.n_query, *all_images.shape[1:])\n",
    "        ))\n",
    "\n",
    "    def get_labels(self) -> List[int]:\n",
    "        return self.labels\n",
    "    \n",
    "    @staticmethod\n",
    "    def _cast_input_data_to_tensor_int_tuple(\n",
    "        input_data: List[Tuple[Tensor, Union[Tensor, int]]]\n",
    "    ) -> List[Tuple[Tensor, int]]:\n",
    "        return [(image, int(label)) for (image, label) in input_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy Sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterator, Optional, Sized, Dict\n",
    "from torch.utils.data import Sampler\n",
    "\n",
    "class DummySampler(Sampler):\n",
    "    \n",
    "    def __init__(self, dataset, n_way, n_shot, n_query, n_tasks, data_source: Sized | None = None) -> None:\n",
    "        super().__init__(data_source)\n",
    "        self.n_way = n_way # number of classes in one task\n",
    "        self.n_shot = n_shot # number of support images for each class in one task\n",
    "        self.n_query = n_query # number of query images for each class in one task\n",
    "        self.n_tasks = n_tasks # number of tasks to sample (each bundle of support and query)\n",
    "\n",
    "        self.items_per_label: Dict[int,List[int]] = {}\n",
    "\n",
    "        for item, label in enumerate(dataset.get_labels()):\n",
    "            if label not in self.items_per_label:\n",
    "                self.items_per_label[label] = [item]\n",
    "            else:\n",
    "                self.items_per_label[label].append(item)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_tasks\n",
    "    \n",
    "    def __iter__(self) -> Iterator[List[int]]:\n",
    "        yield torch.cat([\n",
    "            torch.tensor(\n",
    "                    random.sample(\n",
    "                        self.items_per_label[label], self.n_shot + self.n_query # getting n_shot + n_query random images from selected label\n",
    "                    )\n",
    "                )\n",
    "                for label in random.sample(\n",
    "                    sorted(self.items_per_label.keys()), self.n_way # samples n_way classes\n",
    "                )\n",
    "        ]).tolist() # returns a list of n_way*(n_shot+n_query) numbers representing image indexes within entire dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/UCMerced-Test directory exists.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Dummy_dataset' object has no attribute 'items_per_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m test_set \u001b[39m=\u001b[39m Dummy_dataset(test_set)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m test_sampler \u001b[39m=\u001b[39m DummySampler(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     test_set, n_way\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, n_shot\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, n_query\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, n_tasks\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m test_set\u001b[39m.\u001b[39mepisodic_collate_fn(test_set)\n",
      "\u001b[1;32m/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb Cell 9\u001b[0m line \u001b[0;36m7\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mepisodic_collate_fn\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m     \u001b[39mself\u001b[39m, input_data: List[Tuple[Tensor, Union[Tensor, \u001b[39mint\u001b[39m]]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=69'>70</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Tensor, Tensor, Tensor, List[\u001b[39mint\u001b[39m]]:\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=70'>71</a>\u001b[0m     \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=71'>72</a>\u001b[0m     \u001b[39m# cast label of each data image to an integer\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=72'>73</a>\u001b[0m     input_data_with_int_labels \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cast_input_data_to_tensor_int_tuple(input_data)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=74'>75</a>\u001b[0m     \u001b[39m# true_class_ids is a list from [0-max_class_label] ex: [0,1,2,3,4,5]\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=75'>76</a>\u001b[0m     true_class_ids \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m({x[\u001b[39m1\u001b[39m] \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m input_data_with_int_labels})\n",
      "\u001b[1;32m/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cast_input_data_to_tensor_int_tuple\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     input_data: List[Tuple[Tensor, Union[Tensor, \u001b[39mint\u001b[39m]]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Tuple[Tensor, \u001b[39mint\u001b[39m]]:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [(image, \u001b[39mint\u001b[39m(label)) \u001b[39mfor\u001b[39;00m (image, label) \u001b[39min\u001b[39;00m input_data]\n",
      "\u001b[1;32m/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb Cell 9\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=88'>89</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cast_input_data_to_tensor_int_tuple\u001b[39m(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m     input_data: List[Tuple[Tensor, Union[Tensor, \u001b[39mint\u001b[39m]]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Tuple[Tensor, \u001b[39mint\u001b[39m]]:\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m [(image, \u001b[39mint\u001b[39m(label)) \u001b[39mfor\u001b[39;00m (image, label) \u001b[39min\u001b[39;00m input_data]\n",
      "\u001b[1;32m/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb Cell 9\u001b[0m line \u001b[0;36m6\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m \u001b[39mSample n_way labels uniformly at random,\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=49'>50</a>\u001b[0m \u001b[39mand then sample n_shot + n_query items for each label, also uniformly at random.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m \u001b[39mYields:\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=51'>52</a>\u001b[0m \u001b[39m    a list of indices of length (n_way * (n_shot + n_query))\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=52'>53</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=53'>54</a>\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_tasks):\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=54'>55</a>\u001b[0m     \u001b[39myield\u001b[39;00m torch\u001b[39m.\u001b[39mcat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=55'>56</a>\u001b[0m         [\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=56'>57</a>\u001b[0m             torch\u001b[39m.\u001b[39mtensor(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=57'>58</a>\u001b[0m                 random\u001b[39m.\u001b[39msample(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=58'>59</a>\u001b[0m                     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems_per_label[label], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_shot \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_query\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m                 )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=60'>61</a>\u001b[0m             )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m             \u001b[39mfor\u001b[39;00m label \u001b[39min\u001b[39;00m random\u001b[39m.\u001b[39msample(\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m                 \u001b[39msorted\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems_per_label\u001b[39m.\u001b[39mkeys()), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_way\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m             )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m         ]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/eileen/Documents/Pytorch-Projects/Few_shot_learning/learning_sandbox.ipynb#X11sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m     )\u001b[39m.\u001b[39mtolist()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Dummy_dataset' object has no attribute 'items_per_label'"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "\n",
    "# Setup path to data folder\n",
    "data_path = Path(\"data\")\n",
    "image_path = data_path / \"UCMerced-Test\"\n",
    "\n",
    "# Check if image folder exists\n",
    "if image_path.is_dir():\n",
    "    print(f\"{image_path} directory exists.\")\n",
    "else:\n",
    "    print(f\"Did not find {image_path} directory\")\n",
    "    exit()\n",
    "\n",
    "# Setup train and testing paths\n",
    "test_dir = image_path / \"Test\"\n",
    "\n",
    "# Write transform for image\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize([int(image_size * 1.15), int(image_size * 1.15)]),\n",
    "    transforms.CenterCrop(image_size),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "test_set = datasets.ImageFolder(\n",
    "    root=test_dir,\n",
    "    transform=test_transform,\n",
    ")\n",
    "\n",
    "test_set = Dummy_dataset(test_set)\n",
    "\n",
    "test_sampler = DummySampler(\n",
    "    test_set, n_way=5, n_shot=5, n_query=10, n_tasks=100\n",
    ")\n",
    "\n",
    "test_set.episodic_collate_fn(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Episodic Collate Function\n",
    "\n",
    "Collate function to be used as argument for the collate_fn parameter of episodic data loaders.\n",
    "\n",
    "Args:\n",
    "\n",
    "input_data: each element is a tuple containing:\n",
    "- an image as a torch Tensor of shape (n_channels, height, width)\n",
    "- the label of this image as an int or a 0-dim tensor\n",
    "        \n",
    "Returns:\n",
    "\n",
    "tuple(Tensor, Tensor, Tensor, Tensor, list[int]): respectively:\n",
    "- support images of shape (n_way * n_shot, n_channels, height, width),\n",
    "- their labels of shape (n_way * n_shot),\n",
    "- query images of shape (n_way * n_query, n_channels, height, width)\n",
    "- their labels of shape (n_way * n_query),\n",
    "- the dataset class ids of the class sampled in the episode"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
